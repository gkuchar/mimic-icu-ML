{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adb59a0e",
        "outputId": "1f154ef4-f530-422e-e8cd-be4a7fb06f77"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ast\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "LOADED = True\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Specify the folder path in Google Drive\n",
        "folder_path = '/content/drive/MyDrive/mimic_csvs'\n",
        "\n",
        "# Individual csv paths\n",
        "admissions_path = os.path.join(folder_path, 'ADMISSIONS.csv')\n",
        "icu_stays_path = os.path.join(folder_path, 'ICUSTAYS.csv')\n",
        "patients_path = os.path.join(folder_path, 'PATIENTS.csv')\n",
        "chart_events_path = os.path.join(folder_path, 'CHARTEVENTS.csv')\n",
        "complete_data_path = os.path.join(folder_path, 'mimic_complete_data.csv')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters and outcome\n",
        "# hadm_id as primary key to predict by hosptial visit rather than by patient\n",
        "\n",
        "if not LOADED:\n",
        "  data = {\n",
        "    'hadm_id': [],\n",
        "    'subject_id': [],\n",
        "    'gender': [],\n",
        "    'age': [],\n",
        "    'marital_status': [],\n",
        "    'ethnicity': [],\n",
        "    'diagnosis': [],\n",
        "    'vitals': [],\n",
        "    'icu_admitted': []\n",
        "  }\n"
      ],
      "metadata": {
        "id": "f7l3WnlXG7Af"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load hadm_id, subject_id, ethnicity, marital_status, diagnosis from ADMISSIONS.CSV\n",
        "\n",
        "if not LOADED:\n",
        "  if not os.path.exists(admissions_path):\n",
        "    raise FileNotFoundError(f\"File not found: {admissions_path}\")\n",
        "\n",
        "  admissions_df = pd.read_csv(admissions_path)\n",
        "  data['hadm_id'] = admissions_df['HADM_ID'].tolist()\n",
        "  data['ethnicity'] = admissions_df['ETHNICITY'].tolist()\n",
        "  data['diagnosis'] = admissions_df['DIAGNOSIS'].tolist()\n",
        "  data['marital_status'] = admissions_df['MARITAL_STATUS'].tolist()\n",
        "  data['subject_id'] = admissions_df['SUBJECT_ID'].tolist()\n",
        "\n",
        "  row_count = total_data_points = len(data['hadm_id'])\n",
        "\n",
        "  # Load gender, age (at time of visit) from PATIENTS.CSV\n",
        "  if not os.path.exists(patients_path):\n",
        "    raise FileNotFoundError(f\"File not found: {patients_path}\")\n",
        "\n",
        "  patients_df = pd.read_csv(patients_path)\n",
        "  for i in range(row_count):\n",
        "    subject_id = data['subject_id'][i]\n",
        "    hadm_id = data['hadm_id'][i]\n",
        "\n",
        "    gender = patients_df[patients_df['SUBJECT_ID'] == subject_id]['GENDER'].values[0]\n",
        "    data['gender'].append(gender)\n",
        "\n",
        "    dob = patients_df[patients_df['SUBJECT_ID'] == subject_id]['DOB'].values[0]\n",
        "    admittime = admissions_df[admissions_df['HADM_ID'] == hadm_id]['ADMITTIME'].values[0]\n",
        "\n",
        "    dob_dt = datetime.strptime(str(dob)[:10], '%Y-%m-%d')\n",
        "    admit_dt = datetime.strptime(str(admittime)[:10], '%Y-%m-%d')\n",
        "\n",
        "    age = (admit_dt - dob_dt).days // 365  # Convert days to years\n",
        "\n",
        "    data['age'].append(age)\n",
        "\n",
        "  # Load ICU stays\n",
        "  if not os.path.exists(icu_stays_path):\n",
        "    raise FileNotFoundError(f\"File not found: {icu_stays_path}\")\n",
        "\n",
        "  icustays_df = pd.read_csv(icu_stays_path)\n",
        "  icu_hadm_ids = set(icustays_df['HADM_ID'].tolist())\n",
        "\n",
        "  # Check if each hadm_id is in the ICU stays\n",
        "  data['icu_admitted'] = [hadm_id in icu_hadm_ids for hadm_id in data['hadm_id']]"
      ],
      "metadata": {
        "id": "fMOj3-63LCSU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Vitals Data - First 24 hours after hospital admission time\n",
        "# Each data['vitals'] entry format = [heart_rate_mean, sbp_mean, dbp_mean, temp_mean, resp_rate_mean, spo2_mean]\n",
        "\n",
        "if not LOADED:\n",
        "  if not os.path.exists(chart_events_path):\n",
        "    raise FileNotFoundError(f\"File not found: {chart_events_path}\")\n",
        "\n",
        "  data['vitals'] = [None] * row_count\n",
        "\n",
        "  # Vital sign ITEMIDs corresponding to required vitals parameters\n",
        "  vital_itemids = {\n",
        "      'heart_rate': [211, 220045],\n",
        "      'systolic_bp': [51, 442, 455, 6701, 220179, 220050],\n",
        "      'diastolic_bp': [8368, 8440, 8441, 8555, 220180, 220051],\n",
        "      'temp': [223761, 678],\n",
        "      'resp_rate': [618, 615, 220210, 224690],\n",
        "      'spo2': [646, 220277]\n",
        "  }\n",
        "\n",
        "  # Flatten all vital ITEMIDs into one list\n",
        "  all_vital_ids = [item for items in vital_itemids.values() for item in items]\n",
        "\n",
        "  # Load CHARTEVENTS in chunks\n",
        "  # Filter rows by keeping only parameter vital types and hadm_ids in data\n",
        "  chunk_size = 500000\n",
        "  filtered_chunks = []\n",
        "\n",
        "  print(\"Loading CHARTEVENTS in chunks...\")\n",
        "  for i, chunk in enumerate(pd.read_csv(chart_events_path, chunksize=chunk_size)):\n",
        "      print(f\"Processing chunk {i+1}...\")\n",
        "\n",
        "      # Keep only rows with parameter vital type ITEMIDs\n",
        "      chunk_filtered = chunk[chunk['ITEMID'].isin(all_vital_ids)]\n",
        "\n",
        "      # Keep only rows for hadm_ids in data\n",
        "      chunk_filtered = chunk_filtered[chunk_filtered['HADM_ID'].isin(data['hadm_id'])]\n",
        "      filtered_chunks.append(chunk_filtered)\n",
        "\n",
        "  # Combine all filtered chunks\n",
        "  vitals_df = pd.concat(filtered_chunks, ignore_index=True)\n",
        "  print(f\"Loaded {len(vitals_df)} vital sign records\")\n",
        "\n",
        "  # Convert times to datetime format\n",
        "  vitals_df['CHARTTIME'] = pd.to_datetime(vitals_df['CHARTTIME'])\n",
        "  admissions_df['ADMITTIME'] = pd.to_datetime(admissions_df['ADMITTIME'])\n",
        "\n",
        "  # Merge admission times onto vitals (one-time operation)\n",
        "  vitals_df = vitals_df.merge(\n",
        "      admissions_df[['HADM_ID', 'ADMITTIME']],\n",
        "      on='HADM_ID',\n",
        "      how='left'\n",
        "  )\n",
        "\n",
        "  # Filter to first 24 hours\n",
        "  vitals_df['time_diff_hours'] = (vitals_df['CHARTTIME'] - vitals_df['ADMITTIME']).dt.total_seconds() / 3600\n",
        "  vitals_df_24hr = vitals_df[(vitals_df['time_diff_hours'] >= 0) & (vitals_df['time_diff_hours'] <= 24)]\n",
        "\n",
        "  # Map ITEMIDs to vital names\n",
        "  itemid_to_vital = {}\n",
        "  for vital_name, item_ids in vital_itemids.items():\n",
        "      for item_id in item_ids:\n",
        "          itemid_to_vital[item_id] = vital_name\n",
        "\n",
        "  vitals_df_24hr['vital_type'] = vitals_df_24hr['ITEMID'].map(itemid_to_vital)\n",
        "\n",
        "  # Group and calculate means (vectorized)\n",
        "  vitals_summary = vitals_df_24hr.groupby(['HADM_ID', 'vital_type'])['VALUENUM'].mean().unstack(fill_value=None)\n",
        "\n",
        "  # Ensure columns are in correct order\n",
        "  vital_order = ['heart_rate', 'systolic_bp', 'diastolic_bp', 'temp', 'resp_rate', 'spo2']\n",
        "  vitals_summary = vitals_summary.reindex(columns=vital_order)\n",
        "\n",
        "  # Fill data['vitals'] in correct order\n",
        "  for i in range(row_count):\n",
        "      hadm_id = data['hadm_id'][i]\n",
        "      if hadm_id in vitals_summary.index:\n",
        "          data['vitals'][i] = vitals_summary.loc[hadm_id].tolist()\n",
        "      else:\n",
        "          data['vitals'][i] = [None] * 6"
      ],
      "metadata": {
        "id": "aCAQqC63UUTB",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_eval(x):\n",
        "    try:\n",
        "        return ast.literal_eval(x) if pd.notna(x) else [None]*6\n",
        "    except:\n",
        "        return [None]*6"
      ],
      "metadata": {
        "id": "MMb-yCbN0sr7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View Data Frame and save complete dataset to CSV\n",
        "\n",
        "if not LOADED:\n",
        "  df = pd.DataFrame(data)\n",
        "  df.to_csv(complete_data_path, index=False)\n",
        "  print('data saved to mimic_complete_data.csv')\n",
        "else:\n",
        "  df = pd.read_csv(complete_data_path)\n",
        "  df['vitals'] = df['vitals'].apply(safe_eval)\n",
        "  print('data loaded from mimic_complete_data.csv')\n",
        "\n",
        "total_data_points = len(df)\n",
        "column_count = len(df.columns)\n",
        "print(f\"Total number of data points: {total_data_points}\\n\\nData Frame:\")\n",
        "print(df)"
      ],
      "metadata": {
        "id": "SJz7XShFS1RJ",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47df1e82-bbf6-4035-cb2c-e361e9a62567"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data loaded from mimic_complete_data.csv\n",
            "Total number of data points: 58976\n",
            "\n",
            "Data Frame:\n",
            "       hadm_id  subject_id gender  age marital_status ethnicity  \\\n",
            "0       165315          22      F   64        MARRIED     WHITE   \n",
            "1       152223          23      M   71        MARRIED     WHITE   \n",
            "2       124321          23      M   75        MARRIED     WHITE   \n",
            "3       161859          24      M   39         SINGLE     WHITE   \n",
            "4       129635          25      M   58        MARRIED     WHITE   \n",
            "...        ...         ...    ...  ...            ...       ...   \n",
            "58971   191113       98800      F   19         SINGLE     WHITE   \n",
            "58972   101071       98802      F   83        WIDOWED     WHITE   \n",
            "58973   122631       98805      M   42        MARRIED     WHITE   \n",
            "58974   170407       98813      F   60        MARRIED     WHITE   \n",
            "58975   190264       98813      F   63        MARRIED     WHITE   \n",
            "\n",
            "                                               diagnosis  \\\n",
            "0                                BENZODIAZEPINE OVERDOSE   \n",
            "1      CORONARY ARTERY DISEASE\\CORONARY ARTERY BYPASS...   \n",
            "2                                             BRAIN MASS   \n",
            "3                         INTERIOR MYOCARDIAL INFARCTION   \n",
            "4                                ACUTE CORONARY SYNDROME   \n",
            "...                                                  ...   \n",
            "58971                                             TRAUMA   \n",
            "58972                                                SAH   \n",
            "58973                                   RENAL CANCER/SDA   \n",
            "58974                                           S/P FALL   \n",
            "58975                            INTRACRANIAL HEMORRHAGE   \n",
            "\n",
            "                                                  vitals  icu_admitted  \n",
            "0      [101.875, 141.53846153846155, 74.3076923076923...          True  \n",
            "1      [92.19354838709677, 104.0, 58.16129032258065, ...          True  \n",
            "2                   [None, None, None, None, None, None]          True  \n",
            "3      [68.0909090909091, 119.25, 71.75, 97.799999237...          True  \n",
            "4      [72.87096774193549, 96.88888888888889, 45.0277...          True  \n",
            "...                                                  ...           ...  \n",
            "58971  [89.31818181818181, 113.85, 57.25, 98.75, 19.4...          True  \n",
            "58972  [85.46153846153847, 152.53846153846155, 80.384...          True  \n",
            "58973  [109.23076923076923, 130.07692307692307, 75.53...          True  \n",
            "58974               [None, None, None, None, None, None]          True  \n",
            "58975  [95.84, 108.55555555555556, 61.833333333333336...          True  \n",
            "\n",
            "[58976 rows x 9 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning and Preprocess\n",
        "\n",
        "# PHASE A: CLEANING\n",
        "\n",
        "# 0) Handle missing main category data\n",
        "df = df[df['diagnosis'].notna()]\n",
        "df['marital_status'] = df['marital_status'].fillna('Not Disclosed') # Treat missing marital status as new category \"Not Disclosed\"\n",
        "\n",
        "# 1) Convert list of vitals into separate columns\n",
        "vitals_df = pd.DataFrame(df['vitals'].tolist(),\n",
        "                         columns=['hr_mean', 'sbp_mean', 'dbp_mean',\n",
        "                                 'temp_mean', 'rr_mean', 'spo2_mean'],\n",
        "                         index=df.index)\n",
        "\n",
        "# 2) Create a singular X (the inputs) and y (the target)\n",
        "X = pd.concat([df[['age', 'gender', 'marital_status', 'ethnicity', 'diagnosis']], vitals_df], axis=1)\n",
        "y = df['icu_admitted'].astype(int)\n",
        "\n",
        "# 3) Split the data before statistical learning to prevent data leakage\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "\n",
        "# PHASE B: PREPROCESSING\n",
        "\n",
        "# 4) Process data differently based on their data type, this handles imputation, scaling, encoding, and vectorizing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        # Impute vitals and then scale them\n",
        "        ('vitals_num', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), ['hr_mean', 'sbp_mean', 'dbp_mean', 'temp_mean', 'rr_mean', 'spo2_mean']),\n",
        "\n",
        "        # Scale age\n",
        "        ('age_num', StandardScaler(), ['age']),\n",
        "\n",
        "        # One-Hot Encode categories\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['gender', 'marital_status', 'ethnicity']),\n",
        "\n",
        "        # Vectorize diagnosis text\n",
        "        ('text', TfidfVectorizer(max_features=100, stop_words='english'), 'diagnosis')\n",
        "    ])\n",
        "\n",
        "# 5) Fit on training, and transform both\n",
        "X_train_final = preprocessor.fit_transform(X_train)\n",
        "X_test_final = preprocessor.transform(X_test)"
      ],
      "metadata": {
        "id": "fo7DejLaaisT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Row count after processing = {len(df)}\")\n",
        "for column in df.columns:\n",
        "    if column == 'vitals':\n",
        "      none_count = df[column].apply(lambda x: None in x if isinstance(x, list) else True).sum()\n",
        "    else:\n",
        "      none_count = df[column].isna().sum() + df[column].apply(lambda x: x is None).sum()\n",
        "    print(f\"{column}: {none_count} None entries\")"
      ],
      "metadata": {
        "id": "GFGmNHT5tAwZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2532f486-6a69-4a05-99ba-e9068cfc6093"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row count after processing = 58951\n",
            "hadm_id: 0 None entries\n",
            "subject_id: 0 None entries\n",
            "gender: 0 None entries\n",
            "age: 0 None entries\n",
            "marital_status: 0 None entries\n",
            "ethnicity: 0 None entries\n",
            "diagnosis: 0 None entries\n",
            "vitals: 23247 None entries\n",
            "icu_admitted: 0 None entries\n"
          ]
        }
      ]
    }
  ]
}